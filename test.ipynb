{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent initialized\n",
      "Child initialized\n",
      "10\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "class Parent:\n",
    "    def __init__(self):\n",
    "        self.name = \"Parent\"\n",
    "        self.gender = \"male\"\n",
    "        print(\"Parent initialized\")\n",
    "\n",
    "class Child(Parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.age = 10\n",
    "        print(\"Child initialized\")\n",
    "\n",
    "child_instance = Child()\n",
    "# print(child_instance.name)  # Outputs: Parent\n",
    "print(child_instance.age)  # Outputs: 10\n",
    "print(child_instance.gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big model Inference\n",
    "# https://huggingface.co/docs/accelerate/en/usage_guides/big_modeling\n",
    "# https://webcache.googleusercontent.com/search?q=cache:https://medium.com/geekculture/pipelines-for-performant-inferences-with-hugging-face-5140300522de\n",
    "\n",
    "# Faster Inference\n",
    "# https://huggingface.co/docs/transformers/en/perf_infer_gpu_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.2.81.tar.gz (50.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\nilay kumar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\nilay kumar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python) (1.26.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\nilay kumar\\appdata\\roaming\\python\\python311\\site-packages (from llama-cpp-python) (5.6.1)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\nilay kumar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nilay kumar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.2)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [20 lines of output]\n",
      "      \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.9.8\u001b[0m using \u001b[94mCMake 3.30.0\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "      \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2024-07-06 20:37:38,313 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\NILAYK~1\\AppData\\Local\\Temp\\tmpw593r_yf\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      CMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \n",
      "      \u001b[91m\u001b[1m*** CMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\Nilay Kumar\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m llama_cpp.server --host 0.0.0.0 --port 6000 --hf_model_repo_id Qwen/Qwen2-0.5B-Instruct-GGUF --model '*q8_0.gguf'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = 'path_to_your_trained_model'  # Update this path to your model's directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "def get_model_response(query):\n",
    "    inputs = tokenizer.encode(query, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=512)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Streamlit page configuration\n",
    "st.set_page_config(page_title=\"Doc-GPT Medical Inquiry\", page_icon=\":hospital:\", layout=\"wide\")\n",
    "\n",
    "# Streamlit application\n",
    "def main():\n",
    "    st.title(\"Doc-GPT Medical Inquiry Interface\")\n",
    "    \n",
    "    # Sidebar for query parameters\n",
    "    st.sidebar.header(\"Query Parameters\")\n",
    "    max_length = st.sidebar.slider(\"Max Length of Response\", 50, 512, 150, 10)\n",
    "\n",
    "    # Text box for user input\n",
    "    user_input = st.text_area(\"Enter your medical question here:\", height=150)\n",
    "\n",
    "    # Button to generate response\n",
    "    if st.button(\"Generate Response\"):\n",
    "        if user_input:\n",
    "            response = get_model_response(user_input)\n",
    "            st.text_area(\"Model Response:\", value=response, height=200, max_chars=None)\n",
    "        else:\n",
    "            st.warning(\"Please enter a question to get a response.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
